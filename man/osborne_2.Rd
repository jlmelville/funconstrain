% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/19_osborne_2.R
\name{osborne_2}
\alias{osborne_2}
\title{Osborne 2 Function}
\usage{
osborne_2()
}
\value{
A list containing:
\itemize{
  \item \code{fn} Objective function which calculates the value given input
  parameter vector.
  \item \code{gr} Gradient function which calculates the gradient vector
  given input parameter vector.
  \item \code{fg} A function which, given the parameter vector, calculates
  both the objective value and gradient, returning a list with members
  \code{fn} and \code{gr}, respectively.
  \item \code{x0} Standard starting point.
}
}
\description{
Test function 19 from the More', Garbow and Hillstrom paper.
}
\details{
The objective function is the sum of \code{m} functions, each of \code{n}
parameters.

\itemize{
  \item Dimensions: Number of parameters \code{n = 11}, number of summand
  functions \code{m = 65}.
  \item Minima: \code{f = 4.01377...e-2}.
}
}
\examples{
fun <- osborne_2()
# Optimize using the standard starting point
x0 <- fun$x0
res_x0 <- stats::optim(par = x0, fn = fun$fn, gr = fun$gr, method =
"L-BFGS-B")
# Use your own starting point
res <- stats::optim(1:11, fun$fn, fun$gr, method = "L-BFGS-B")
}
\references{
More', J. J., Garbow, B. S., & Hillstrom, K. E. (1981).
Testing unconstrained optimization software.
\emph{ACM Transactions on Mathematical Software (TOMS)}, \emph{7}(1), 17-41.
\url{https://doi.org/10.1145/355934.355936}

Osborne, M. R. (1972).
Some aspects of nonlinear least squares calculations.
In F. A. Lootsma (Ed.),
\emph{Numerical methods for nonlinear optimization} (pp. 171-189).
New York, NY: Academic Press.
}
