% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/10_meyer.R
\name{meyer}
\alias{meyer}
\title{Meyer Function}
\usage{
meyer()
}
\value{
A list containing:
\itemize{
  \item \code{fn} Objective function which calculates the value given input
  parameter vector.
  \item \code{gr} Gradient function which calculates the gradient vector
  given input parameter vector.
  \item \code{he} If available, the hessian matrix (second derivatives)
  of the function w.r.t. the parameters at the given values.
  \item \code{fg} A function which, given the parameter vector, calculates
  both the objective value and gradient, returning a list with members
  \code{fn} and \code{gr}, respectively.
  \item \code{x0} Standard starting point.
}
}
\description{
Test function 10 from the More', Garbow and Hillstrom paper.
}
\details{
The objective function is the sum of \code{m} functions, each of \code{n}
parameters.

\itemize{
  \item Dimensions: Number of parameters \code{n = 3}, number of summand
  functions \code{m = 16}.
  \item Minima: the MGH (1981) only provides the optimal value, with \code{f
  = 87.9458...}. Meyer and Roth (1972) give the optimal parameter values as
  \code{(0.0056, 6181.4, 345.2)}, with \code{f = 88}.
}
}
\note{
The gradient is large even at the optimal value, and enormous if using the
level of precision given by Meyer and Roth (smallest gradient component is at
least 1e4). It is not recommended to rely on the typical gradient norm
termination conditions if using this test function.
}
\examples{
fun <- meyer()
# Optimize using the standard starting point
x0 <- fun$x0
res_x0 <- stats::optim(par = x0, fn = fun$fn, gr = fun$gr, method =
"L-BFGS-B")
# Use your own starting point
res <- stats::optim(c(0.1, 0.2, 0.3), fun$fn, fun$gr, method = "L-BFGS-B")
}
\references{
More', J. J., Garbow, B. S., & Hillstrom, K. E. (1981).
Testing unconstrained optimization software.
\emph{ACM Transactions on Mathematical Software (TOMS)}, \emph{7}(1), 17-41.
\doi{doi.org/10.1145/355934.355936}

Meyer, R. R. (1970).
Theoretical and computational aspects of nonlinear regression.
In J. B. Rosen, O. L. Mangasarian, and K. Ritter (Eds.)
\emph{Nonlinear programming} (pp465-496).
New York: Academic Press.

Meyer, R. R., & Roth, P. M. (1972).
Modified damped least squares: an algorithm for non-linear estimation.
\emph{IMA Journal of Applied Mathematics}, \emph{9}(2), 218-233.
\doi{doi.org/10.1093/imamat/9.2.218}
}
